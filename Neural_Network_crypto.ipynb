{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e86cb-ea5a-4893-ad53-6ecf5add3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the packages\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Conv1D, Flatten, MaxPooling1D, RepeatVector, \\\n",
    "    TimeDistributed, LayerNormalization, Dropout, MultiHeadAttention, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import statsmodels.api as sm\n",
    "from pmdarima import auto_arima\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a218d-8ef7-47a1-b240-9bbb0e904306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for measuring the performance\n",
    "import quantstats as qs\n",
    "import yfinance as yf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824b016-eb92-471c-86dc-6904162ac68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3e673-e91c-4e9d-964b-fb5902a021d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input data\n",
    "def read_data(path, dim_type, gold_data=None, fed_rate=None, use_percentage=1):\n",
    "    df = pd.read_csv(path)\n",
    "    data_len = df.shape[0]\n",
    "    data = None\n",
    "    if dim_type!='Multi':\n",
    "        data = df[dim_type].values.reshape((data_len, 1))\n",
    "    else:\n",
    "        # Multi\n",
    "        df[\"Date\"]=pd.to_datetime(df[\"Date\"], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\n",
    "        #df['CO'] = df['Close'] - df['Open']\n",
    "        #diff_data = df['CO'].values.reshape((data_len, 1))\n",
    "        open_data = df[\"Open\"].values.reshape((data_len, 1))\n",
    "        high_data = df[\"High\"].values.reshape((data_len, 1))\n",
    "        low_data = df[\"Low\"].values.reshape((data_len, 1))\n",
    "        close_data = df[\"Close\"].values.reshape((data_len, 1))\n",
    "        volume_data = df[\"Volume\"].values.reshape((data_len, 1))\n",
    "        # input gold price\n",
    "        if gold_data is not None:\n",
    "            gold_data['Date']=gold_data['Date'].dt.strftime('%Y-%m-%d')\n",
    "            df['Gold']=df['Date']\n",
    "            \n",
    "            # calc the gold series\n",
    "            df['Gold'] = df['Gold'].apply(lambda x: gold_data['Close'][x==gold_data['Date']].values[0] if x in gold_data['Date'].values else np.nan)  \n",
    "            # fillna using interpolating\n",
    "            df['Gold'] = df['Gold'].interpolate(limit_direction=\"both\")\n",
    "\n",
    "            gold_data_fill=df[\"Gold\"].values.reshape((data_len, 1))\n",
    "            #input the federal rate \n",
    "            if fed_rate is not None:\n",
    "                fed_rate['DATE']=fed_rate['DATE'].dt.strftime('%Y-%m-%d')\n",
    "                df['fed']=df['Date']\n",
    "                \n",
    "                # calc the gold series\n",
    "                df['fed'] = df['fed'].apply(lambda x: fed_rate['FEDFUNDS'][x==fed_rate['DATE']].values[0] if x in fed_rate['DATE'].values else np.nan)  \n",
    "                # fillna using interpolating\n",
    "                df['fed'] = df['fed'].interpolate(limit_direction=\"both\")\n",
    "    \n",
    "                fed_data=df[\"fed\"].values.reshape((data_len, 1))\n",
    "                data = np.hstack((close_data, open_data, high_data, low_data, volume_data, gold_data_fill, fed_data))\n",
    "            else:\n",
    "                data = np.hstack((close_data, open_data, high_data, low_data, volume_data, gold_data_fill))\n",
    "        else:\n",
    "            data = np.hstack((close_data, open_data, high_data, low_data, volume_data))\n",
    "    return data[0:int(np.floor(data_len * use_percentage))], np.floor(data_len * use_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5140c9-ab56-43d7-a4b0-72e27e0ae271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X and Y value\n",
    "def split_sequence(sequence, dim_type, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of the input pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        # find the end of the output pattern\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        if dim_type == 'Multi':\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x = sequence[i:end_ix, 1:]\n",
    "            seq_y = sequence[end_ix:out_end_ix, 0]\n",
    "        else:\n",
    "            seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b211c-6665-4825-809b-f88fc8c946d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data \n",
    "def data_trasform(data, anti=False, scaler=None):\n",
    "    '''\n",
    "    说明以及例子\n",
    "    MinMax data and anti MinMax data\n",
    "    :param data: the data source\n",
    "    :param model: MinMax and anti MinMax\n",
    "    :param scaler: anti MinMax scaler\n",
    "    :return: the transformed data\n",
    "\n",
    "    '''\n",
    "    if not anti:\n",
    "        # 归一化\n",
    "        # 创建一个空字典来存储每一列的 scaler\n",
    "        scalers = {}\n",
    "        # 归一化数据的容器\n",
    "        normalized_data = np.zeros_like(data)\n",
    "        # 循环每一列\n",
    "        for i in range(data.shape[1]):  # data.shape[1] 是列的数量\n",
    "            # 为每一列创建一个新的 MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "            # 将列数据调整为正确的形状，即(-1, 1)\n",
    "            column_data = data[:, i].reshape(-1, 1)\n",
    "            # 拟合并转换数据\n",
    "            normalized_column = scaler.fit_transform(column_data)\n",
    "            # 将归一化的数据存回容器中\n",
    "            normalized_data[:, i] = normalized_column.ravel()\n",
    "            # 存储scaler以便后续使用\n",
    "            scalers[i] = scaler\n",
    "        # 现在 normalized_data 是完全归一化的数据\n",
    "        # scalers 字典包含每一列的 MinMaxScaler 实例\n",
    "        return normalized_data, scalers\n",
    "    else:\n",
    "        # 反归一化\n",
    "        # 如果data是三维数组，去除最后一个维度\n",
    "        if data.ndim == 3 and data.shape[2] == 1:\n",
    "            data = data.squeeze(axis=2)\n",
    "\n",
    "        restored_data = np.zeros_like(data)\n",
    "        for i in range(data.shape[1]):  # 遍历所有列\n",
    "            column_data = data[:, i].reshape(-1, 1)\n",
    "            restored_data[:, i] = scaler.inverse_transform(column_data).ravel()\n",
    "        return restored_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa05bb-85e9-4b2b-b436-3199b3378b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type, n_features, n_steps_in, n_steps_out):\n",
    "    '''\n",
    "        create model\n",
    "        :param model_type:  LSTM,BD LSTM(bidirectional LSTM),ED LSTM(Encoder-Decoder LSTM),CNN\n",
    "        :param n_features:\n",
    "        :param n_steps_in:\n",
    "        :param n_steps_out:\n",
    "        :return: the created model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    adam_optimizer = Adam(learning_rate=0.001)\n",
    "    if model_type == 'LSTM':\n",
    "        # LSTM\n",
    "        model.add(LSTM(100, activation='sigmoid', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "        model.add(LSTM(100, activation='sigmoid'))\n",
    "        model.add(Dense(n_steps_out))\n",
    "\n",
    "    elif model_type == 'BD LSTM':\n",
    "        # bidirectional LSTM\n",
    "        model.add(Bidirectional(LSTM(50, activation='sigmoid'), input_shape=(n_steps_in, n_features)))\n",
    "        model.add(Dense(n_steps_out))\n",
    "\n",
    "    elif model_type == 'ED LSTM':\n",
    "        # Encoder-Decoder LSTM\n",
    "        # Encoder\n",
    "        model.add(LSTM(100, activation='sigmoid', input_shape=(n_steps_in, n_features)))\n",
    "        # Connector\n",
    "        model.add(RepeatVector(n_steps_out))\n",
    "        # Decoder\n",
    "        model.add(LSTM(100, activation='sigmoid', return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "    elif model_type == 'CNN':\n",
    "        # CNN\n",
    "        model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(n_steps_out))\n",
    "\n",
    "    elif model_type == 'Convolutional LSTM':\n",
    "        # Convolutional LSTM\n",
    "        model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "        model.add(LSTM(20, activation='relu', return_sequences=False))\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(n_steps_out))\n",
    "\n",
    "    elif model_type == 'Transformer':\n",
    "        model = create_transformer_model(n_steps_in, n_steps_out, n_features, d_model=64,\n",
    "                                         num_heads=12, ff_dim=64, num_transformer_blocks=3)\n",
    "\n",
    "    elif model_type == 'MLP':\n",
    "        # 多层感知机 (MLP)\n",
    "        model.add(Dense(20, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(n_steps_out))\n",
    "\n",
    "    elif model_type == 'ARIMA':\n",
    "        if n_features != 1:\n",
    "            print(\"ARIMA model only supports univariate time series data\")\n",
    "            print(\"ARIMA model has no parameter n_steps_in\")\n",
    "            return None\n",
    "        model = 'ARIMA'\n",
    "        return model\n",
    "    \n",
    "    else:\n",
    "        print(\"no model\")\n",
    "    model.compile(optimizer=adam_optimizer, loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0206a-d77d-4e21-b29d-9fc3caeee5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_forecast(model, n_features, dim_type, data_X, data_Y, n_steps_in, n_steps_out, ech):\n",
    "    # train the model \n",
    "    # hide the output \n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    X, y = split_sequence(data_X, dim_type, n_steps_in, n_steps_out)\n",
    "    # 对于多维数据，调整最后一个维度为特征数\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "#######################################################################################\n",
    "    if model == 'ARIMA':\n",
    "        # # 自动确定 ARIMA 模型的参数\n",
    "        # auto_model = auto_arima(data_X, seasonal=False, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "        # # 输出最佳 ARIMA 模型的参数\n",
    "        # print(auto_model.summary())\n",
    "        \n",
    "        # 使用最佳参数拟合 ARIMA 模型\n",
    "        # ARIMA 模型只接受单变量时间序列，这里假设 data_X 和 data_Y 是一维数组\n",
    "        # order = auto_model.order\n",
    "        order = (6,1,6)\n",
    "        arma_model = ARIMA(data_X, order=order)\n",
    "        model_fit = arma_model.fit()\n",
    "        # 拟合结果\n",
    "        fit_result = model_fit.fittedvalues\n",
    "        # 使用模型进行滚动预测\n",
    "        history = list(data_X)\n",
    "        test_result = []\n",
    "        for t in range(len(data_Y)):\n",
    "            model = ARIMA(history, order=order)\n",
    "            model_fit = model.fit()\n",
    "            output = model_fit.forecast(n_steps_out)\n",
    "            test_result.append(output)\n",
    "            history.append(data_Y[t])  # update the history state\n",
    "        test_result = np.array(test_result)\n",
    "        fit_result = fit_result.reshape(len(fit_result), 1)\n",
    "        return fit_result, test_result\n",
    "#######################################################################################\n",
    "\n",
    "    # traing the model\n",
    "    model.fit(X, y, epochs=ech, batch_size=32, verbose=1)\n",
    "\n",
    "    # store the fitting result\n",
    "    fit_result = []\n",
    "    for index, ele in enumerate(X):\n",
    "        print(f'Fitting {index}th data')\n",
    "        pred = model.predict(ele.reshape((1, n_steps_in, n_features)))\n",
    "        fit_result.append(pred)\n",
    "    fr = np.array(fit_result)\n",
    "    fit_result = fr.reshape(len(fit_result), n_steps_out)\n",
    "    # show the test result\n",
    "    test_x, test_y = split_sequence(data_Y, dim_type, n_steps_in, n_steps_out)\n",
    "    test_x = test_x.reshape((test_x.shape[0], test_x.shape[1], n_features))\n",
    "    test_result = []\n",
    "    for index, ele in enumerate(test_x):\n",
    "        print(f'Predicting {index}th data')\n",
    "        pred = model.predict(ele.reshape((1, n_steps_in, n_features)))\n",
    "        test_result.append(pred)\n",
    "    tr = np.array(test_result)\n",
    "    test_result = tr.reshape(len(test_result), n_steps_out)\n",
    "\n",
    "    # 恢复输出\n",
    "    sys.stdout = sys.__stdout__\n",
    "    sys.stderr = sys.__stderr__\n",
    "    return fit_result, test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a758b-97d7-4e6a-94aa-a07e15be63f3",
   "metadata": {},
   "source": [
    "# Predict Close Price using 6 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b48010-6dde-4eb3-a7a6-e8b1f07e66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = yf.download(tickers=\"GC=F\")\n",
    "\n",
    "start = datetime(2013, 4, 29)\n",
    "end = datetime(2021, 7, 6)\n",
    "\n",
    "filtered = gold[start: end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6d971-7c74-43b6-b284-4fa1cec0e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_rate = pd.read_csv('FEDFUNDS.csv')\n",
    "fed_rate[\"DATE\"] = pd.to_datetime(fed_rate[\"DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab830ddd-2893-4499-8c03-c03974eb573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'coin_Bitcoin.csv'\n",
    "gold_data = None \n",
    "fed_rate = None \n",
    "dim_type = 'Multi'\n",
    "use_percentage = 1\n",
    "\n",
    "n_steps_in = 5  # take prior 5 values as input \n",
    "n_steps_out = 1 # predict t+1 value \n",
    "\n",
    "percentage = 0.7  # 训练集百分比\n",
    "epochs = 25  # 迭代次数\n",
    "#rounds = 3  # Number of exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ad6ce-b7f8-4bd6-a980-992c78adbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_len = read_data(path, dim_type, gold_data, fed_rate, use_percentage)\n",
    "data, scalers = data_trasform(data)\n",
    "\n",
    "# split into train and test\n",
    "train_set = data[0:int(np.floor(data_len * percentage))]  # train\n",
    "test_set = data[int(np.floor(data_len * percentage)):]  # test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beec119-90c0-4520-ac82-bdd5b53e83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the test dataset for the backtesting\n",
    "test_bit_data = bit_data[int(np.floor(data_len * percentage)):]\n",
    "update_dataset_model = test_bit_data[n_steps_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d78224-1838-41a7-8dfd-219faa60222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the number of features\n",
    "n_features = len(train_set[0]) - 1 if len(train_set[0]) > 1 else 1 # features of the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9166a2f-40a5-4015-bac6-28017b1058e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the model type\n",
    "model_type =['LSTM', 'BD LSTM', 'CNN', 'MLP', 'ED LSTM', 'Convolutional LSTM']\n",
    "\n",
    "pred_name = []\n",
    "for i in model_type:\n",
    "    pred_name.append('prediction_'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316c07b-b6e8-4956-b2f0-1dcd22e7e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_type)):\n",
    "    model_use = model_type[i]\n",
    "    Model = create_model(model_use, n_features, n_steps_in, n_steps_out)\n",
    "    \n",
    "    train_result, test_result = train_and_forecast(Model, n_features, dim_type, train_set, test_set, n_steps_in,\n",
    "                                                    n_steps_out, epochs)\n",
    "    \n",
    "    # ----------------------evaluation--------------------\n",
    "    train_result = data_trasform(train_result, True, scalers[0])  # 反归一化\n",
    "    test_result = data_trasform(test_result, True, scalers[0])  # 反归一化\n",
    "    \n",
    "    test_result_list = test_result.tolist()\n",
    "    t_list = list()\n",
    "    for j in range(len(test_result_list)):\n",
    "        number = test_result_list[j][0]\n",
    "        t_list.append(number)\n",
    "    update_dataset_model[pred_name[i]] = t_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a69989-39a2-4f0d-a116-a056f589544c",
   "metadata": {},
   "source": [
    "# Predict Close Price Using Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3badf-4e7d-4ff6-9bb2-25295be0e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'coin_Bitcoin.csv'\n",
    "gold_data_ARIMA = None  \n",
    "fed_rate_ARIMA = None \n",
    "dim_type_ARIMA = 'Close' #ARIMA ony accepts one dimensional data\n",
    "use_percentage = 1\n",
    "\n",
    "n_steps_in = 5  # take prior 5 values as input \n",
    "n_steps_out = 1 # predict t+1 value \n",
    "\n",
    "percentage = 0.7  # 训练集百分比\n",
    "epochs = 25  # 迭代次数\n",
    "#rounds = 3  # Number of exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67b8a2-888c-4eb8-a55c-017824a669ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ARIMA, data_len_ARIMA = read_data(path, dim_type_ARIMA, gold_data_ARIMA, fed_rate_ARIMA, use_percentage)\n",
    "data_ARIMA, scalers_ARIMA = data_trasform(data_ARIMA)\n",
    "\n",
    "# split into train and test\n",
    "train_set_ARIMA = data_ARIMA[0:int(np.floor(data_len_ARIMA * percentage))]  \n",
    "test_set_ARIMA = data_ARIMA[int(np.floor(data_len_ARIMA * percentage)):]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc9440-7de1-46d0-ab9d-db5857ad179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_ARIMA = len(train_set_ARIMA[0]) - 1 if len(train_set_ARIMA[0]) > 1 else 1 # features of the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc340934-0fb5-47d8-b694-d5e1c79f40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ARIMA' # ARIMA\n",
    "\n",
    "Model = create_model(model_type, n_features_ARIMA, n_steps_in, n_steps_out)\n",
    "\n",
    "order = (6,1,6)\n",
    "arma_model = ARIMA(train_set_ARIMA, order=order)\n",
    "model_fit = arma_model.fit()\n",
    "# 拟合结果\n",
    "fit_result = model_fit.fittedvalues\n",
    "# 使用模型进行滚动预测\n",
    "history = list(train_set_ARIMA)\n",
    "test_result = []\n",
    "for t in range(len(test_set_ARIMA)):\n",
    "    model = ARIMA(history, order=order)\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast(n_steps_out)\n",
    "    test_result.append(output)\n",
    "    history.append(test_set_ARIMA[t])  \n",
    "test_result = np.array(test_result)\n",
    "fit_result = fit_result.reshape(len(fit_result), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef2400-952f-495b-8dc0-16f671a8a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------evaluation--------------------\n",
    "test_result_ARIMA = data_trasform(test_result, True, scalers[0])  # denormalize\n",
    "\n",
    "test_result_list_ARIMA = test_result_ARIMA.tolist()\n",
    "t_list_ARIMA = list()\n",
    "for j in range(len(test_result_list_ARIMA)):\n",
    "    number = test_result_list_ARIMA[j][0]\n",
    "    t_list_ARIMA.append(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16de22-19c1-4a2e-b862-3a692dec4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dataset_model['Prediction_ARIMA'] = t_list_ARIMA_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612de6e-9176-4108-b46a-c8d6e258e6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f051e0-39a6-4073-9f1c-ee14fcff9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the prediction result\n",
    "update_dataset_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
